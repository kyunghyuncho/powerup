!obj:pylearn2.train.Train {
    dataset: &train !obj:pylearn2.datasets.mnist.MNIST {
        which_set: 'train',
        one_hot: 1,
    },
    model: !obj:pylearn2.models.mlp.MLP {
        batch_size: &batch_size 128,
        layers: [
                 !obj:pylearn2.models.powerup.Powerup {
                     layer_name: 'h0',
                     num_units: 240,
                     num_pieces: 12,
                     W_lr_scale: 0.2,
                     max_col_norm: 2.1365,
                     irange: .005,
                     post_bias: True,
                     batch_size: *batch_size
                 },
                 !obj:pylearn2.models.maxout.Maxout {
                     layer_name: 'h1',
                     num_units: 240,
                     num_pieces: 4,
                     W_lr_scale: 0.5,
                     b_lr_scale: 0.5,
                     max_col_norm: 1.9365,
                     irange: .005
                 },
                 !obj:pylearn2.models.maxout.Maxout {
                     layer_name: 'h2',
                     num_units: 240,
                     num_pieces: 4,
                     W_lr_scale: 0.5,
                     b_lr_scale: 0.5,
                     max_col_norm: 1.8365,
                     irange: .005
                 },
                 !obj:pylearn2.models.powerup.Powerup {
                     layer_name: 'h3',
                     num_units: 240,
                     num_pieces: 5,
                     W_lr_scale: 0.4,
                     p_lr_scale: 0.65,
                     max_col_norm: 2.1365,
                     irange: .005,
                     post_bias: True,
                     batch_size: *batch_size
                 },
                 !obj:pylearn2.models.mlp.Softmax {
                     max_col_norm: 1.6365,
                     layer_name: 'y',
                     n_classes: 10,
                     sparse_init: 15
                 }
                ],
        nvis: 784
    },
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        learning_rate: .25,
        learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {
            init_momentum: .5,
        },
        cost: !obj:pylearn2.costs.cost.SumOfCosts {
            costs: [
                !obj:pylearn2.costs.mlp.dropout.Dropout {
                    input_include_probs: { 'h1' : .4 },
                    input_scales: { 'h1': 1. }
                },
                !obj:pylearn2.costs.mlp.WeightDecay {
                    coeffs: [0.00005, 0.0000, 0.00000, 0.000005, 0.0001]
                }
            ]
        },
        monitoring_dataset:
            {   'train': *train,
                'test' : !obj:pylearn2.datasets.mnist.MNIST {
                              which_set: 'test',
                              one_hot: 1,
                 },
            },
        termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter {
                max_epochs: 400
        },
        update_callbacks: !obj:pylearn2.training_algorithms.sgd.ExponentialDecay {
            decay_factor: 1.00004,
            min_lr: .000001
        }
    },
    extensions: [
        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
             channel_name: 'test_y_misclass',
             save_path: "powerup_mnist_best.pkl"
        },
        !obj:pylearn2.training_algorithms.learning_rule.MomentumAdjustor {
            start: 1,
            saturate: 250,
            final_momentum: .7
        }
    ],
    save_path: "powerup_last.pkl",
    save_freq: 5
}
