----------------------------------------
Begin PBS Prologue Mon Oct 21 18:48:53 EDT 2013 1382395733
Job ID:		5204[112].hades
Username:	gulcehre
Group:		lisa
Nodes:		ngpu-a4-03
End PBS Prologue Mon Oct 21 18:48:53 EDT 2013 1382395733
----------------------------------------
book_unstarted_dct retrieved,  Dict{u'decay_factor': 0.0103168568776101, u'jobman.experiment': u'powconvnet.train_pow.experiment', u'max_col_norm2': 1.8365, u'jobman.sql.priority': 1.0, u'yaml_string': u'!obj:pylearn2.train.Train {\n    dataset: &train !obj:pylearn2.datasets.tfd.TFD {\n        which_set: \'train\',\n        one_hot: 1,\n        center: 1,\n        preprocessor: !obj:pylearn2.datasets.preprocessing.Pipeline {\n            items: [ \n                       !obj:pylearn2.datasets.preprocessing.GlobalContrastNormalization {}\n                   ]\n        }\n    },\n    model: !obj:pylearn2.models.mlp.MLP {\n        batch_size: 128,\n        layers: [\n                !obj:pylearn2.models.powerup.Powerup {\n                     layer_name: \'h0\',\n                     num_units: %(powerup_nunits)i,\n                     num_pieces: %(powerup_npieces)i,\n                     post_bias: True,\n                     p_mean: 1.0,\n                     W_lr_scale: %(W_lr_scale)f,\n                     p_lr_scale: %(p_lr_scale)f,\n                     max_col_norm: %(max_col_norm)f,\n                     batch_size: 128,\n                     irange: .005,\n                 },\n                 !obj:pylearn2.models.mlp.Softmax {\n                     max_col_norm: %(max_col_norm2)f,\n                     layer_name: \'y\',\n                     n_classes: 7,\n                     irange: .005\n                 }\n                ],\n        nvis: 2304\n    },\n    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {\n        learning_rate: %(lr_rate)f,\n        monitoring_dataset:\n            {\n                \'train\' : *train,\n                \'valid\' : !obj:pylearn2.datasets.tfd.TFD {\n                              which_set: \'valid\',\n                              one_hot: 1,\n                              center: 1,\n                              preprocessor: !obj:pylearn2.datasets.preprocessing.Pipeline {\n                                  items: [\n                                       !obj:pylearn2.datasets.preprocessing.GlobalContrastNormalization {}\n                                   ]\n                              }\n                          },\n                \'test\' : !obj:pylearn2.datasets.tfd.TFD {\n                              which_set: \'test\',\n                              one_hot: 1,\n                              center: 1,\n                              preprocessor: !obj:pylearn2.datasets.preprocessing.Pipeline {\n                                  items: [\n                                       !obj:pylearn2.datasets.preprocessing.GlobalContrastNormalization {}\n                                   ]\n                              }\n                          }\n            },\n        cost: !obj:pylearn2.costs.cost.SumOfCosts { \n            costs: [\n                !obj:pylearn2.costs.cost.MethodCost {\n                    method: \'cost_from_X\'\n                },\n                !obj:pylearn2.models.mlp.WeightDecay {\n                    coeffs: [ %(l2_pen)f, %(l2_pen2)f ]\n                }\n            ],\n        },\n       termination_criterion: !obj:pylearn2.termination_criteria.MonitorBased {\n            channel_name: "valid_y_misclass",\n            prop_decrease: 0.,\n            N: 100\n        },\n    },\n    extensions: [\n        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {\n             channel_name: \'valid_y_misclass\',\n             save_path: "%(save_path)sbest.pkl"\n        },\n        !obj:pylearn2.training_algorithms.sgd.LinearDecayOverEpoch {\n            start: 1,\n            saturate: 250,\n            decay_factor: %(decay_factor)f\n        }\n    ],\n    save_path: "%(save_path)slast.pkl",\n    save_freq: 5\n}\n', u'powerup_npieces': 6, u'jobman.hash': -4123964673643239247, u'l2_pen2': 0.0, u'p_lr_scale': 0.60979541239225, u'l2_pen': 6.96527850057654e-06, u'max_col_norm': 2.1365, u'jobman.status': 0, u'W_lr_scale': 1.09197776970114, u'save_path': u'./abcdefg_', u'lr_rate': 0.0861953566475303, u'powerup_nunits': 200}
caught exception (TransactionRollbackError) could not serialize access due to concurrent update
 'UPDATE powerup_tfd_1layer_finer_large_no_momtrial SET status=%(status)s WHERE powerup_tfd_1layer_finer_large_no_momtrial.id = %(powerup_tfd_1layer_finer_large_no_momtrial_id)s' {'powerup_tfd_1layer_finer_large_no_momtrial_id': 111, 'status': 1}
another process stole our dct. Waiting 7.979196 secs
waiting for 7 second
book_unstarted_dct retrieved,  Dict{u'decay_factor': 0.0186600433475531, u'jobman.experiment': u'powconvnet.train_pow.experiment', u'max_col_norm2': 1.8365, u'jobman.sql.priority': 1.0, u'yaml_string': u'!obj:pylearn2.train.Train {\n    dataset: &train !obj:pylearn2.datasets.tfd.TFD {\n        which_set: \'train\',\n        one_hot: 1,\n        center: 1,\n        preprocessor: !obj:pylearn2.datasets.preprocessing.Pipeline {\n            items: [ \n                       !obj:pylearn2.datasets.preprocessing.GlobalContrastNormalization {}\n                   ]\n        }\n    },\n    model: !obj:pylearn2.models.mlp.MLP {\n        batch_size: 128,\n        layers: [\n                !obj:pylearn2.models.powerup.Powerup {\n                     layer_name: \'h0\',\n                     num_units: %(powerup_nunits)i,\n                     num_pieces: %(powerup_npieces)i,\n                     post_bias: True,\n                     p_mean: 1.0,\n                     W_lr_scale: %(W_lr_scale)f,\n                     p_lr_scale: %(p_lr_scale)f,\n                     max_col_norm: %(max_col_norm)f,\n                     batch_size: 128,\n                     irange: .005,\n                 },\n                 !obj:pylearn2.models.mlp.Softmax {\n                     max_col_norm: %(max_col_norm2)f,\n                     layer_name: \'y\',\n                     n_classes: 7,\n                     irange: .005\n                 }\n                ],\n        nvis: 2304\n    },\n    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {\n        learning_rate: %(lr_rate)f,\n        monitoring_dataset:\n            {\n                \'train\' : *train,\n                \'valid\' : !obj:pylearn2.datasets.tfd.TFD {\n                              which_set: \'valid\',\n                              one_hot: 1,\n                              center: 1,\n                              preprocessor: !obj:pylearn2.datasets.preprocessing.Pipeline {\n                                  items: [\n                                       !obj:pylearn2.datasets.preprocessing.GlobalContrastNormalization {}\n                                   ]\n                              }\n                          },\n                \'test\' : !obj:pylearn2.datasets.tfd.TFD {\n                              which_set: \'test\',\n                              one_hot: 1,\n                              center: 1,\n                              preprocessor: !obj:pylearn2.datasets.preprocessing.Pipeline {\n                                  items: [\n                                       !obj:pylearn2.datasets.preprocessing.GlobalContrastNormalization {}\n                                   ]\n                              }\n                          }\n            },\n        cost: !obj:pylearn2.costs.cost.SumOfCosts { \n            costs: [\n                !obj:pylearn2.costs.cost.MethodCost {\n                    method: \'cost_from_X\'\n                },\n                !obj:pylearn2.models.mlp.WeightDecay {\n                    coeffs: [ %(l2_pen)f, %(l2_pen2)f ]\n                }\n            ],\n        },\n       termination_criterion: !obj:pylearn2.termination_criteria.MonitorBased {\n            channel_name: "valid_y_misclass",\n            prop_decrease: 0.,\n            N: 100\n        },\n    },\n    extensions: [\n        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {\n             channel_name: \'valid_y_misclass\',\n             save_path: "%(save_path)sbest.pkl"\n        },\n        !obj:pylearn2.training_algorithms.sgd.LinearDecayOverEpoch {\n            start: 1,\n            saturate: 250,\n            decay_factor: %(decay_factor)f\n        }\n    ],\n    save_path: "%(save_path)slast.pkl",\n    save_freq: 5\n}\n', u'powerup_npieces': 5, u'jobman.hash': -3225841150075724959, u'l2_pen2': 0.0, u'p_lr_scale': 0.416002425286193, u'l2_pen': 3.23791010691916e-05, u'max_col_norm': 1.8365, u'jobman.status': 0, u'W_lr_scale': 0.800220049674781, u'save_path': u'./abcdefg_', u'lr_rate': 0.00243835409826883, u'powerup_nunits': 320}
Selected job id=114 in table=powerup_tfd_1layer_finer_large_no_mom in db=gulcehrc_db
----------------------------------------
Begin PBS Epilogue Mon Oct 21 19:00:57 EDT 2013 1382396457
Job ID:		5204[112].hades
Username:	gulcehre
Group:		lisa
Job Name:	dbi_d1f3b0e21b2-112
Session:	1136
Limits:		neednodes=1:ppn=1,nodes=1:ppn=1,walltime=47:59:59
Resources:	cput=00:10:58,mem=571192kb,vmem=47570708kb,walltime=00:11:59
Queue:		courte
Account:		
Nodes:	ngpu-a4-03
Killing leftovers...

End PBS Epilogue Mon Oct 21 19:00:58 EDT 2013 1382396458
----------------------------------------
